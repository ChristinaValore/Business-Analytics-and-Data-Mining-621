install.packages("pROC")
data<- read.csv("https://raw.githubusercontent.com/hvasquez81/DATA621/master/classification-output-data.csv")
head(data) %>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = TRUE, position = "center", font_size = 9)
library(tidyr); library(dplyr); library(kableExtra); library(pROC); library(caret)
library(tidyr); library(dplyr); library(kableExtra); library(pROC); library(caret)
data<- read.csv("https://raw.githubusercontent.com/hvasquez81/DATA621/master/classification-output-data.csv")
head(data) %>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = TRUE, position = "center", font_size = 9)
data<- read.csv("https://raw.githubusercontent.com/hvasquez81/DATA621/master/classification-output-data.csv")
head(data) %>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = TRUE, position = "center", font_size = 15)
#row: predicted value; columns: actual value
conf_matrix = table(Prediction = data$scored.class, Actual = data$class)
conf_matrix %>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = TRUE, position = "center", font_size = 9)
#row: predicted value; columns: actual value
conf_matrix = table(Prediction = data$scored.class, Actual = data$class)
conf_matrix %>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = TRUE, position = "center", font_size = 15)
accuracy = function(data, predicted_col_name, actual_col_name) {
conf = table(data[ , predicted_col_name], data[ , actual_col_name])
TP = conf[2,2]
TN = conf[1,1]
FP = conf[2,1]
FN = conf[1,2]
#Accurary = (TP + TN) / (TP + FP +TN +FN)
return(round((TP+TN)/(TP + FP + TN + FN), 4))
}
print(paste0("Accuracy: ", accuracy(data, 'scored.class', 'class')))
errorRate = function(data, predicted_col_name, actual_col_name) {
conf = table(data[ , predicted_col_name], data[ , actual_col_name])
TP = conf[2,2]
TN = conf[1,1]
FP = conf[2,1]
FN = conf[1,2]
#Classification Error Rate = ( FP + FN )/(TP + FP +TN +FN)
return(round((FP+FN)/(TP + FP + TN + FN), 4))
}
print(paste0("Error rate: ", errorRate(data, 'scored.class', 'class')))
#accuracy + error rate
print(paste0("Accuracy + Error rate = ", accuracy(data, 'scored.class', 'class'), " + ", errorRate(data, 'scored.class', 'class'), " = ", (accuracy(data, 'scored.class', 'class') + errorRate(data, 'scored.class', 'class'))))
precision = function(data, predicted_col_name, actual_col_name) {
conf = table(data[ , predicted_col_name], data[ , actual_col_name])
TP = conf[2,2]
TN = conf[1,1]
FP = conf[2,1]
FN = conf[1,2]
#Precision = TP / (TP + FP)
return(round((TP)/(TP + FP), 4))
}
print(paste0("Precision: ", precision(data, 'scored.class', 'class')))
sensitivity = function(data, predicted_col_name, actual_col_name) {
conf = table(data[ , predicted_col_name], data[ , actual_col_name])
TP = conf[2,2]
TN = conf[1,1]
FP = conf[2,1]
FN = conf[1,2]
#Sensitivity = TP / (TP + FN)
return(round((TP)/(TP + FN), 4))
}
print(paste0("Sensitivity: ", sensitivity(data, 'scored.class', 'class')))
specificity = function(data, predicted_col_name, actual_col_name) {
conf = table(data[ , predicted_col_name], data[ , actual_col_name])
TP = conf[2,2]
TN = conf[1,1]
FP = conf[2,1]
FN = conf[1,2]
#Specificity = TN / (TN+FP)
return(round((TN)/(TN + FP), 4))
}
print(paste0("Specificity: ", specificity(data, 'scored.class', 'class')))
f1_score = function(data, predicted_col_name, actual_col_name) {
Precision = precision(data, predicted_col_name, actual_col_name)
Sensitivity = sensitivity(data, predicted_col_name, actual_col_name)
#F1 Score = 2 * Precision * Sensitivity / (Precision + Sensitivity)
return(round((2*Precision*Sensitivity)/(Precision + Sensitivity), 4))
}
print(paste0("F1 score: ", f1_score(data, 'scored.class', 'class')))
print(paste0("Accuracy: ", accuracy(data, 'scored.class', 'class')))
print(paste0("Error rate: ", errorRate(data, 'scored.class', 'class')))
print(paste0("Accuracy + Error rate = ", accuracy(data, 'scored.class', 'class'), " + ", errorRate(data, 'scored.class', 'class'), " = ", (accuracy(data, 'scored.class', 'class') + errorRate(data, 'scored.class', 'class'))))
print(paste0("Precision: ", precision(data, 'scored.class', 'class')))
print(paste0("Sensitivity: ", sensitivity(data, 'scored.class', 'class')))
print(paste0("Specificity: ", specificity(data, 'scored.class', 'class')))
print(paste0("F1 score: ", f1_score(data, 'scored.class', 'class')))
confusionMatrix(conf_matrix, positive = '1')
caret::sensitivity(conf_matrix, positive = '1')
caret::specificity(conf_matrix, negative = '0')
confusionMatrix(conf_matrix, positive = '1')
sensitivity(conf_matrix, positive = '1')
print(paste0("Accuracy: ", accuracy(data, 'scored.class', 'class')))
print(paste0("Error rate: ", errorRate(data, 'scored.class', 'class')))
print(paste0("Accuracy + Error rate = ", accuracy(data, 'scored.class', 'class'), " + ", errorRate(data, 'scored.class', 'class'), " = ", (accuracy(data, 'scored.class', 'class') + errorRate(data, 'scored.class', 'class'))))
print(paste0("Precision: ", precision(data, 'scored.class', 'class')))
print(paste0("Sensitivity: ", sensitivity(data, 'scored.class', 'class')))
print(paste0("Specificity: ", specificity(data, 'scored.class', 'class')))
print(paste0("F1 score: ", f1_score(data, 'scored.class', 'class')))
confusionMatrix(conf_matrix, positive = '1')
sensitivity(conf_matrix, positive = '1')
confusionMatrix(conf_matrix, positive = '1')
caret::sensitivity(conf_matrix, positive = '1')
specificity(conf_matrix, negative = '0')
confusionMatrix(conf_matrix, positive = '1')
caret::sensitivity(conf_matrix, positive = '1')
caret::specificity(conf_matrix, negative = '0')
library(tidyr); library(dplyr); library(kableExtra); library(pROC); library(caret)
roc(data$class, data$scored.probability, plot = TRUE)
