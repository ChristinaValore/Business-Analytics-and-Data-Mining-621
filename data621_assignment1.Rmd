---
title: 'Assignment 1: Data 621'
author: "Critical Thinking Group 5 - Christina Valore, Henry Vasquez, Chunhui Zhu, Chunmei Zhu"
date: "9/16/2019"
output: html_document
---

```{r,message=F, warning=F}
library(tidyverse); library(dplyr); library(psych); library(corrplot); library (ggplot2); library(funModeling); library(Hmisc); library (car); library(caTools)
```

## Money Ball

In this homework assignment, you will explore, analyze and model a data set containing approximately 2200 records. Each record represents a professional baseball team from the years 1871 to 2006 inclusive. Each record has the performance of the team for the given year, with all of the statistics adjusted to match the performance of a 162 game season.

Your objective is to build a multiple linear regression model on the training data to predict the number of wins for the team. You can only use the variables given to you (or variables that you derive from the variables provided). Below is a short description of the variables of interest in the data set:

Response variable: Target_Wins
Predictor variable(s): Target_Wins = (team batting base hits + team batting walks) - (team pitching hits allowed + team pitching walks allowed)

### Data Analysis

First, we import data from our GitHub repo, then we remove any unnecessary columns such as 'index', which plays no role in our analysis. Once completed, we view the top of each dataset to ensure data was imported in the correct format.

```{r }
# import data
data <-read.csv("https://raw.githubusercontent.com/ChristinaValore/Business-Analytics-and-Data-Mining-621/master/moneyball-training-data.csv") 
eval <-read.csv("https://raw.githubusercontent.com/ChristinaValore/Business-Analytics-and-Data-Mining-621/master/moneyball-evaluation-data.csv") 

data <- select(data,-1) # remove index column, which is first column in both datasets
eval <- select(eval,-1)

head (data) # view data for both sets, notice the index column has been removed
head (eval)
```

We now will split the training data set into two sets: train_1 and test_1. The test_1 will help us validate our model. We will save the eval set to run predictions.
```{r}
set.seed(123)   #  set seed to ensure you always have same random numbers generated
sample <- sample.split(data,SplitRatio = 0.80) 
train_1 <- subset(data, sample ==TRUE) 
test_1 <- subset(data, sample==FALSE)

dim(train_1)
dim(test_1)
```

### Statistics

The summary function reveals there are a considerable amount of NA's and some variables have a min of 0, which can be possible in baseball, however a variable with too many zeros will be unuseful. Additonally some variables have max values that look far off from the 3rd quartile, i.e. team_batting_3B has the 3rd q at 72, while max is 223. These could be possible outliers.

```{r}
summary(train_1) # univariate view of basic stats
```


### Zeros and NA values

Using the df_status function from the funModeling package, we see that the quantity of zeros (q_zeros) is quite low for all variables. This can be confirmed by percentage of zeros (p_zeros) as all fall below 1%. 

The NA values seem to be more discerning as several of the variable have high percentage NA's. Specifically the values, baserun_CS and batting_HBP which has 33.92% and 91.61% NA's respectively. 

```{r}
#gives us information about 0's, NA's, variable type, and unique values
df_status(train_1)
```

These variables will need to be excluded from our analysis as we only want variables with less than 20% NA's. 

```{r}
train_1 <- subset(train_1, select = -c(TEAM_BASERUN_CS,TEAM_BATTING_HBP)) # drop both variable columns from dataframe
head(train_1) # check columns are removed
```


### Graphs

After some research, we decide to investigate the hits, walks and hits allowed and walks allowed as we think these four variables overall will have an impact on the wins.

By examining the scatter plots, we see that team batting base hits and team batting walks seem to have a linear relationship to wins as those variables increase it looks as if wins increase as well. 

Team pitching hits allowed and team pitching walks allowed, are not as clear and may not have a linear relationship to wins. They both also look to have more extreme ouliers as we saw in the summary. 

```{r}
par(mfrow = c(2,2)) #view graphs in 2x2 view

plot(TARGET_WINS ~ TEAM_BATTING_H,train_1) # batting hits

plot(TARGET_WINS ~ TEAM_BATTING_BB,train_1) # batting walks

plot(TARGET_WINS ~ TEAM_PITCHING_H,train_1) # pitching hits allowed

plot(TARGET_WINS ~ TEAM_PITCHING_BB,train_1) # pitching walks allowed

```

Batting_H and Batting_BB look to be the most normally distributed, where pitching_H and pitching_BB seem to be more concetrated to the left again indicating there may not be a linear relationship to target wins. 

```{r}

par(mfrow = c(2,2)) #view graphs in 2x2 view

hist(train_1$TEAM_BATTING_H) # hits 

hist(train_1$TEAM_BATTING_BB) # walks

hist(train_1$TEAM_PITCHING_H) # hits allowed

hist(train_1$TEAM_PITCHING_BB) #walks allowed

```

### Outliers

Since this will be a multi-variable regression, we want to consider how all four variables will affect the wins. This is done by using Cook's distance, which calculates the influence each point (row) has on the predicted outcome. 

```{r}
m <- lm(TARGET_WINS ~ TEAM_BATTING_H + TEAM_BATTING_BB - (TEAM_PITCHING_H + TEAM_PITCHING_BB), data=train_1) # model with four variables considered

cook <- cooks.distance(m)

plot(cook, pch="*", cex=2, main="Cook's Distance: Influential Observations") # plot Cook's distance
abline(h = 4*mean(cook, na.rm=T), col="red") # add cutoff line, which is equivalent to 4*mean
text(x=1:length(cook)+1, y=cook, labels=ifelse(cook>4*mean(cook, na.rm=T),names(cook),""), col="red") # add labels for points (rows)
```

We can see the full view of the influential numbers by adding to a dataframe and viewing the head. We have 66 influential outliers. To take it one step further, we can find out what is the most influential outlier. 

```{r}
influential <- as.numeric(names(cook)[(cook > 4*mean(cook, na.rm=T))]) # add influential rows 

head(train_1[influential, ]) # show rows that match influential numbers
```

The most influential outlier is at row 1828. In the next section we will have to account how to handle the 89 outliers, including row 1828.

```{r}
outlierTest(m)
```

## Data Preparation

### Handling NA values

We impute the missing NA values with the mean using the Hmisc package:

```{r include=FALSE}
train_1$TEAM_BATTING_SO<-impute(train_1$TEAM_BATTING_SO, median)
train_1$TEAM_BASERUN_SB<-impute(train_1$TEAM_BASERUN_SB, median)
train_1$TEAM_PITCHING_SO<-impute(train_1$TEAM_PITCHING_SO, median)
train_1$TEAM_FIELDING_DP<-impute(train_1$TEAM_FIELDING_DP, median)
```


### Handling Outliers

We will have *TWO* train dataframes to compare which one performs better with our testing, one with outliers (train_1) and one with outliers removed (train_no):
```{r}
#removed outliers
train_no<-train_1[-influential, ]
```

### New variables

We create three variables:

team batting on base = team batting base hits + team batting walks
team pitching allows = team pitching hits allowed + team pitching walks allowed
BASE_DIFF = team batting on base - team pitching allows

Our thought is that the team that is on base more, will have a higher chance of winning.

```{r}

# with outliers
train_1$TEAM_BATTING_OB <- train_1$TEAM_BATTING_H + train_1$TEAM_BATTING_BB # create variable team batting on base
train_1$TEAM_PITCHING_A <- train_1$TEAM_PITCHING_H + train_1$TEAM_PITCHING_BB # create variable pitching allows
train_1$BASE_DIFF <- train_1$TEAM_BATTING_OB - train_1$TEAM_PITCHING_A #create variable base diff

# without outliers
train_no$TEAM_BATTING_OB <- train_no$TEAM_BATTING_H + train_no$TEAM_BATTING_BB # create variable team batting on base
train_no$TEAM_PITCHING_A <- train_no$TEAM_PITCHING_H + train_no$TEAM_PITCHING_BB # create variable pitching allows
train_no$BASE_DIFF <- train_no$TEAM_BATTING_OB - train_no$TEAM_PITCHING_A # create variable base diff
```


```{r}

par(mfrow = c(1,2)) #view graphs in 2x2 view
plot(train_1$BASE_DIFF,train_1$TARGET_WINS, xlab = "On plate difference", ylab = "Wins", title("Wins vs. On plate  difference"))

plot(train_no$BASE_DIFF,train_no$TARGET_WINS, xlab = "On plate difference", ylab = "Wins", title("Wins vs. On plate  difference"))

```

## Build Models

```{r model_1}
model_1 = lm(TARGET_WINS ~ TEAM_BATTING_OB + TEAM_BATTING_HR + TEAM_PITCHING_A + TEAM_FIELDING_E + TEAM_FIELDING_DP, data = train_1)

summary(model_1)
```

```{r model_1_no}
model_1_no = lm(TARGET_WINS ~ TEAM_BATTING_OB + TEAM_BATTING_HR + TEAM_PITCHING_A + TEAM_FIELDING_E + TEAM_FIELDING_DP, data = train_no)

summary(model_1_no)
```

```{r model_2}
model_2 = lm(TARGET_WINS ~ TEAM_BATTING_OB + TEAM_FIELDING_E + TEAM_FIELDING_DP, data = train_1)

summary(model_2)
```




```{r model_2_no}
model_2_no = lm(TARGET_WINS ~ TEAM_BATTING_OB+ TEAM_FIELDING_E + TEAM_FIELDING_DP, data = train_no)

summary(model_2_no)
```


```{r model_3}
model_3 = lm(TARGET_WINS ~ TEAM_BATTING_OB + TEAM_FIELDING_E, data = train_1)
summary(model_3)
```


```{r model_3_no}
model_3_no = lm(TARGET_WINS ~ TEAM_BATTING_OB + TEAM_FIELDING_E, data = train_no)
summary(model_3_no)
```




```{r model_4}
model_4 = lm(TARGET_WINS ~ TEAM_BATTING_OB + TEAM_FIELDING_E +TEAM_BASERUN_SB, data = train_1)
summary(model_4)
```


```{r model_4_no}
model_4_no = lm(TARGET_WINS ~ TEAM_BATTING_OB + TEAM_FIELDING_E +TEAM_BASERUN_SB, data = train_no)
summary(model_4_no)
```


## Select Models
Since there it appears that models that exlude the outliers result with lowered R-squared and Adjusted R-squared values, all no models will be excluded from testing.


multicollinearity


##### a) RMSE
```{r mse calc}


#remove same columns from train_1
test_1 <- subset(test_1, select = -c(TEAM_BASERUN_CS,TEAM_BATTING_HBP))

#missing values for test
test_1$TEAM_BATTING_SO<-impute(test_1$TEAM_BATTING_SO, median)
test_1$TEAM_BASERUN_SB<-impute(test_1$TEAM_BASERUN_SB, median)
test_1$TEAM_PITCHING_SO<-impute(test_1$TEAM_PITCHING_SO, median)
test_1$TEAM_FIELDING_DP<-impute(test_1$TEAM_FIELDING_DP, median)


#add for test_1 
test_1$TEAM_BATTING_OB <- test_1$TEAM_BATTING_H + test_1$TEAM_BATTING_BB # create variable team batting on base
test_1$TEAM_PITCHING_A <- test_1$TEAM_PITCHING_H + test_1$TEAM_PITCHING_BB # create variable pitching allows
test_1$BASE_DIFF <- test_1$TEAM_BATTING_OB - test_1$TEAM_PITCHING_A

wins = test_1$TARGET_WINS

as.matrix(c(mean((wins - predict.lm(model_1, newdata = test_1))^2),
            mean((wins - predict.lm(model_2, newdata = test_1))^2),
            mean((wins - predict.lm(model_3, newdata = test_1))^2),
            mean((wins - predict.lm(model_4, newdata = test_1))^2))
)

mse(test_1$TARGET_WINS, model_1_predict)


```

model_1 had the best, followed by 2, then 5 and finally 3.



##### b) R-squared


model_1
issues with model_1 inlcude the double-play variable which suggests good defense but the model suggests more losses - contradicting. There's also 2 estimators that are not significat.

model_2
slightly lowers r-squared, but all variables are now significant. However same issue with model_1 - contradicting value for double-play

model_3
r-squared is definitely lower then before, but the estimators are accurate with their outcomes for the team - i.e more errors = lose more and more on-base opportunities = win more.

model_4
r-squared improved with addition of stolen-bases. all values are significant and suggest correct outcome for team.

Conclusion: although model_4 does not have the highest r-squared value, it does have all of its predictors with significant p-values and correct positive/negative values based on their effect to the team.




##### c) F-statistic
```{r}
summary(model_1)
summary(model_2)
summary(model_3)
summary(model_4)
```


The F-statistic on all models have significant p-values, suggesting the group of variables are jointly significant.


##### d) residual plots

```{r}
plot(model_1)
```


```{r}
plot(model_2)
#residuals vs fitted suggests constant variance
#in normqq plot there appears to be some skewness at the right end
#scale-location shows homoscedasticity 
#residuals vs leverage shows influential points
```


```{r}
plot(model_3)
#residuals vs fitted suggests constant variance
#in normqq plot there appears to be some skewness at the right end
#scale-location shows homoscedasticity 
#residuals vs leverage shows influential points
```



```{r}
plot(model_4)
#residuals vs fitted suggests constant variance
#in normqq plot there appears to be some skewness at the right end
#scale-location shows homoscedasticity 
#residuals vs leverage shows influential points
```



Checking the residuals plots the models all appear to follow to the assumptions of linear regression. 

multicollinearity
```{r}
vif(model_1)
vif(model_2)
vif(model_3)
vif(model_4)

```


the VIF values suggest multicollinearity is not an issue, however it is more apparent in model_1 then in the other models.




##### Make predictions using the evaluation data set
```{r}
#remove same columns from train_1
eval <- subset(eval, select = -c(TEAM_BASERUN_CS,TEAM_BATTING_HBP))

#missing values for test
eval$TEAM_BATTING_SO<-impute(eval$TEAM_BATTING_SO, median)
eval$TEAM_BASERUN_SB<-impute(eval$TEAM_BASERUN_SB, median)
eval$TEAM_PITCHING_SO<-impute(eval$TEAM_PITCHING_SO, median)
eval$TEAM_FIELDING_DP<-impute(eval$TEAM_FIELDING_DP, median)


#add for test_1 
eval$TEAM_BATTING_OB <- eval$TEAM_BATTING_H + eval$TEAM_BATTING_BB # create variable team batting on base
eval$TEAM_PITCHING_A <- eval$TEAM_PITCHING_H + eval$TEAM_PITCHING_BB # create variable pitching allows
eval$BASE_DIFF <- eval$TEAM_BATTING_OB - eval$TEAM_PITCHING_A



predict_eval = round(predict(model_4, eval),0)


```


Sources: 

http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram

https://kharshit.github.io/blog/2017/07/28/moneyball-how-linear-regression-changed-baseball

https://towardsdatascience.com/linear-regression-moneyball-part-1-b93b3b9f5b53

http://r-statistics.co/Outlier-Treatment-With-R.html

https://blog.datascienceheroes.com/exploratory-data-analysis-in-r-intro/

