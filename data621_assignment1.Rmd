---
title: 'Assignment 1: Data 621'
author: "Critical Thinking Group 5 - Christina Valore, Henry Vasquez, Chunhui Zhu, Chunmei Zhu"
date: "9/16/2019"
output: html_document
---

```{r,message=F, warning=F}
library(tidyverse); library(dplyr); library(psych); library(corrplot); library (ggplot2); library(funModeling) 
```

## Money Ball

In this homework assignment, you will explore, analyze and model a data set containing approximately 2200 records. Each record represents a professional baseball team from the years 1871 to 2006 inclusive. Each record has the performance of the team for the given year, with all of the statistics adjusted to match the performance of a 162 game season.

Your objective is to build a multiple linear regression model on the training data to predict the number of wins for the team. You can only use the variables given to you (or variables that you derive from the variables provided). Below is a short description of the variables of interest in the data set:

Response variable: Target_Wins
Predictor variable(s): ?

### Data Analysis

First, we import data from our GitHub repo, then we remove any unnecessary columns such as 'index', which plays no role in our analysis. Once completed, we view the top of each dataset to ensure data was imported in the correct format.

```{r }
# import data
train <-read.csv("https://raw.githubusercontent.com/ChristinaValore/Business-Analytics-and-Data-Mining-621/master/moneyball-training-data.csv") 
eval <-read.csv("https://raw.githubusercontent.com/ChristinaValore/Business-Analytics-and-Data-Mining-621/master/moneyball-evaluation-data.csv") 

train <- select(train,-1) # remove index column, which is first column in both datasets
eval <- select(eval,-1)

head (train) # view data for both sets, notice the index column has been removed
head (eval)

```

### Statistics

The summary function reveals there are a considerable amount of NA's and some variables have a min of 0, which can be possible in baseball, however a variable with too many zeros will be unuseful. Additonally some variables have max values that look far off from the 3rd quartile, i.e. team_batting_3B has the 3rd q at 72, while max is 223. These could be possible outliers.

```{r}
summary(train) # univariate view of basic stats
```

### Zeros and NA values

Using the df_status function from the funModeling package, we see that the quantity of zeros (q_zeros) is quite low for all variables. This can be confirmed by percentage of zeros (p_zeros) as all fall below 1%. 

The NA values seem to be more discerning as several of the variable have high percentage NA's. Specifically the values, baserun_CS and batting_HBP which has 33.92% and 91.61% NA's respectively. 

```{r}
#gives us information about 0's, NA's, variable type, and unique values
df_status(train)
```

These variables will need to be excluded from our analysis as we only want variables with less than 20% NA's. 

```{r}
train <- subset(train, select = -c(TEAM_BASERUN_CS,TEAM_BATTING_HBP)) # drop both variable columns from data frame
head(train) # check columns are removed
```


### Outliers

Since this will be a multi-variable regression, we want to consider how all variables will affect the target_wins variable. This is done using Cook's distance, which calculates the influence each point (row) has on the predicted outcome. 

```{r}
m <- lm(TARGET_WINS ~ ., data=train) # model of data with all variables considered

cook <- cooks.distance(m)

plot(cook, pch="*", cex=2, main="Cook's Distance: Influential Observations") # plot Cook's distance
abline(h = 4*mean(cook, na.rm=T), col="red") # add cutoff line, which is equivalent to 4*mean
text(x=1:length(cook)+1, y=cook, labels=ifelse(cook>4*mean(cook, na.rm=T),names(cook),""), col="red") # add labels for points (rows)
```

We have 84 rows that are influential to the predicted outcome, target_wins. We can take this one step further, using the car package to to find which row is the most influential

```{r}
influential <- as.numeric(names(cook)[(cook > 4*mean(cook, na.rm=T))]) # influential row numbers

head(train[influential, ]) # show rows that match influential numbers
```

We can see the most influential outlier is row 205. In the next section, we will decide on how to handle these 84 outliers. 

```{r}
car::outlierTest(m)
```

### Graphs

Batting_H, batting_2B and batting_HBP look the most normally distributed. 

```{r}

par(mfrow = c(3,3)) #view graphs in 3x3 view

hist(train$TEAM_BATTING_HR) # homerun 

hist(train$TEAM_PITCHING_HR) # homeruns allowed

hist(train$TEAM_BATTING_H) # hits 

hist(train$TEAM_BATTING_2B) # doubles 

hist(train$TEAM_BATTING_3B) # triples 

hist(train$TEAM_BATTING_BB) # walks

hist(train$TEAM_BASERUN_SB) # stolen 

hist(train$TEAM_FIELDING_DP) # double 

hist(train$TEAM_PITCHING_SO) # strikeouts by pitcher

```


## Data Preparation

### Add variable

Not sure how to calculate runs from the given data?
```{r}
train$RUN_DIFF = train$TEAM_BATTING_HR - train$TEAM_PITCHING_HR
```

I wanted to plot here Run diff vs Wins, however this graph looks off, there should be some runs in the positive - again I think we need an equation to calculate overal runs. 
```{r}
plot(train$RUN_DIFF, train$TARGET_WINS, xlab = "Homeruns", ylab = "Wins", title("Wins vs. homeruns"))

```

## Build Models

## Select Models

Sources: 

http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram

https://kharshit.github.io/blog/2017/07/28/moneyball-how-linear-regression-changed-baseball

https://towardsdatascience.com/linear-regression-moneyball-part-1-b93b3b9f5b53

http://r-statistics.co/Outlier-Treatment-With-R.html

https://blog.datascienceheroes.com/exploratory-data-analysis-in-r-intro/

